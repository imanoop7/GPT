# Let's Build GPT from Scratch

Welcome to my GitHub repository where we explore building a GPT model from scratch! In this project, we'll follow the concepts from the "Attention is All You Need" paper.
We will start building form one euron from scratch to whole GPT model.

## Table of Contents
1. Introduction
2. Getting Started
3. Resources

## Introduction
Generatively Pretrained Transformers (GPTs) are powerful language models that have revolutionized natural language understanding and generation. In this project, we'll dive into the details of building our own GPT model.

## Getting Started
Before you begin, make sure you have the following prerequisites:
- Basic knowledge of neural networks and natural language processing and basic maths.
- Python and a deep learning framework (such as PyTorch or TensorFlow)


## Resources
Here are some helpful resources to guide you:
- [YouTube: Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY) by Andrej Karpathy

## Acknowledgments

- This project was inspired by the original GPT model developed by OpenAI.
- Special thanks to Andrej Karpathy for his contributions to the field of deep learning and his educational resources. This repo is based on Andrej Karpathy's `Neural Networks: Zero to Hero` [playlist](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ).


 Happy coding! ðŸš€
